{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922defdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import os\n",
    "from from_scratch.dataset import Dataset, resize\n",
    "from from_scratch.losses import ComputeLoss\n",
    "from from_scratch.model import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950cfc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loader : 4 batches\n",
      "All keys in batch      :  dict_keys(['cls', 'box', 'idx'])\n",
      "Input batch shape      :  torch.Size([32, 1, 640, 640])\n",
      "Classification scores  : torch.Size([135, 1])\n",
      "Box coordinates        : torch.Size([135, 4])\n",
      "Index identifier (which score belongs to which image): torch.Size([135])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('/home/davide/Desktop/change_detection/dataset', input_size=640)\n",
    "train_loader = data.DataLoader(dataset, batch_size=32, num_workers=0, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "print(f\"Train_loader : {len(train_loader)} batches\")\n",
    "batch=next(iter(train_loader))\n",
    "print(\"All keys in batch      : \", batch[1].keys())\n",
    "print(f\"Input batch shape      : \", batch[0].shape)\n",
    "print(f\"Classification scores  : {batch[1]['cls'].shape}\")\n",
    "print(f\"Box coordinates        : {batch[1]['box'].shape}\")\n",
    "print(f\"Index identifier (which score belongs to which image): {batch[1]['idx'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.649366 million parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | loss : 208957.65625\n",
      "Epoch : 1 | loss : 135740.4375\n",
      "Epoch : 2 | loss : 82601.671875\n",
      "Epoch : 3 | loss : 48735.58984375\n",
      "Epoch : 4 | loss : 30216.58203125\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = Yolo(version='n')\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "criterion = ComputeLoss(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "imgs,targets=batch[0],batch[1]\n",
    "imgs=imgs.float()\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    outputs=model(imgs)\n",
    "    loss=sum(criterion(outputs,targets))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch : {epoch + 1} | loss : {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609db924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 640, 640])\n",
      "After conv_0: torch.Size([1, 16, 320, 320])\n",
      "After conv_1: torch.Size([1, 32, 160, 160])\n",
      "After c2f_2: torch.Size([1, 32, 160, 160])\n",
      "After conv_3: torch.Size([1, 64, 80, 80])\n",
      "After c2f_4: torch.Size([1, 64, 80, 80]) <----- out1 shape\n",
      "After conv_5: torch.Size([1, 128, 40, 40])\n",
      "After c2f_6: torch.Size([1, 128, 40, 40]) <----- out2 shape\n",
      "After conv_7: torch.Size([1, 256, 20, 20])\n",
      "After c2f_8: torch.Size([1, 256, 20, 20])\n",
      "After sppf: torch.Size([1, 256, 20, 20])\n",
      "Input shapes: x_res_1: torch.Size([1, 64, 80, 80]), x_res_2: torch.Size([1, 128, 40, 40]), x (res_1): torch.Size([1, 256, 20, 20])\n",
      "After upsample: torch.Size([1, 256, 40, 40])\n",
      "After concatenation with x_res_2: torch.Size([1, 384, 40, 40])\n",
      "After c2f_1: torch.Size([1, 128, 40, 40]) <-- res_2 shape\n",
      "After upsample: torch.Size([1, 128, 80, 80])\n",
      "After concatenation with x_res_1: torch.Size([1, 192, 80, 80])\n",
      "After c2f_2: torch.Size([1, 64, 80, 80]) <----- out_1 shape\n",
      "After cv_1: torch.Size([1, 64, 40, 40])\n",
      "After concatenation with res_2: torch.Size([1, 192, 40, 40])\n",
      "After c2f_3: torch.Size([1, 128, 40, 40]) <----- out_2 shape\n",
      "After cv_2: torch.Size([1, 128, 20, 20])\n",
      "After concatenation with res_1: torch.Size([1, 384, 20, 20])\n",
      "After c2f_4: torch.Size([1, 256, 20, 20]) <----- out_3 shape\n",
      "Head input shapes: [torch.Size([1, 64, 80, 80]), torch.Size([1, 128, 40, 40]), torch.Size([1, 256, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_img = cv2.imread('/home/davide/Desktop/change_detection/dataset/images/val/149.png')\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "h, w = test_img.shape\n",
    "r = 640 / max(h, w)\n",
    "if r != 1:\n",
    "    test_img = cv2.resize(test_img, dsize=(int(w * r), int(h * r)), interpolation=cv2.INTER_LINEAR)\n",
    "test_img, ratio, pad = resize(test_img, 640)\n",
    "h, w = test_img.shape\n",
    "test_img = test_img.reshape((1, 1, h, w))\n",
    "test_img = np.ascontiguousarray(test_img)\n",
    "test_img = torch.from_numpy(test_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(test_img.float())\n",
    "    out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15235ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes shape : torch.Size([6400, 4])\n",
      "Scores shape : torch.Size([6400])\n",
      "Confidences 0 shape : torch.Size([6400])\n",
      "Confidences 1 shape : torch.Size([6400])\n"
     ]
    }
   ],
   "source": [
    "out = out[0].T\n",
    "\n",
    "boxes = out[:, :4]\n",
    "confidences_0 = out[:, 4]\n",
    "confidences_1 = out[:, 5]\n",
    "\n",
    "scores = torch.maximum(confidences_0, confidences_1)\n",
    "mask = scores > 0.6\n",
    "boxes = boxes[mask]\n",
    "scores = scores[mask]\n",
    "confidences_0 = confidences_0[mask]\n",
    "confidences_1 = confidences_1[mask]\n",
    "\n",
    "boxes_xyxy = torch.zeros_like(boxes)\n",
    "boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2\n",
    "boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2\n",
    "boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2\n",
    "boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2\n",
    "print(f\"Boxes shape : {boxes_xyxy.shape}\")\n",
    "print(f\"Scores shape : {scores.shape}\")\n",
    "print(f\"Confidences 0 shape : {confidences_0.shape}\")\n",
    "print(f\"Confidences 1 shape : {confidences_1.shape}\")\n",
    "import torchvision\n",
    "keep = torchvision.ops.nms(boxes_xyxy, scores, iou_threshold=0)\n",
    "boxes_xyxy = boxes_xyxy[keep]\n",
    "scores = scores[keep]\n",
    "confidences_0 = confidences_0[keep]\n",
    "confidences_1 = confidences_1[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63d73e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6400])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815e1b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " tensor([[1.],\n",
       "         [0.]]),\n",
       " tensor([[0.4320, 0.1960, 0.0279, 0.0246],\n",
       "         [0.5459, 0.8270, 0.0262, 0.0311]]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
